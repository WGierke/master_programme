{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning II: $Q$-learning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first reinforcment lab we learned how to find the best value-state function $V^*$ for a given Markov decision process (MDP). In real world problems specifing a complet MDP is often not possible due to unknown or infinite (or at least very big) state space. In such cases Monte Carlo methods can be used to learn $Q$ directly.\n",
    "\n",
    "In this lab we will investigate Q-learning and how to deal with an unknown state space. Therfore we define a class which stores q values in a lookup table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class QValues(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        '''Initialize with empty lookup table.'''\n",
    "        self.values = {}\n",
    "        \n",
    "    def get_value(self, state, action):\n",
    "        '''Return stored q value for (state, action) pair or a random number if unknown.'''\n",
    "        if not state in self.values:\n",
    "            self.values[state] = {}\n",
    "        if not action in self.values[state]:\n",
    "            self.values[state][action] = abs(np.random.randn()) + 1\n",
    "        return self.values[state][action]\n",
    "    \n",
    "    def set_value(self, state, action, value):\n",
    "        '''Stored q value for (state, action) pair.'''\n",
    "        if not state in self.values:\n",
    "            self.values[state] = {}\n",
    "        if not action in self.values[state]:\n",
    "            self.values[state][action] = 0\n",
    "        \n",
    "        self.values[state][action] = value\n",
    "    \n",
    "    def max_action(self, state, actions, learning=True):\n",
    "        '''Return the action with highest q value for given state and action list.'''\n",
    "        if not learning and not state in self.values:\n",
    "            return actions[0] if actions else None\n",
    "        \n",
    "        max_value = -np.inf\n",
    "        max_action = actions[0] if actions else None\n",
    "        for action in actions:\n",
    "            if not learning and not action in self.values[state]:\n",
    "                continue\n",
    "\n",
    "            value = self.get_value(state, action)\n",
    "            if value > max_value:\n",
    "                max_value = value\n",
    "                max_action = action\n",
    "            elif value == max_value and learning:\n",
    "                max_action = np.random.choice([max_action, action])\n",
    "        return max_action\n",
    "    \n",
    "    def epsilon_greedy(self, state, actions, epsilon):\n",
    "        '''Returns max_action or random action with probability of epsilon.'''\n",
    "        if np.random.rand() < epsilon:\n",
    "            return np.random.choice(actions)\n",
    "        return self.max_action(state, actions)\n",
    "    \n",
    "    def __str__(self):\n",
    "        nr_states = len(self.values.keys())\n",
    "        return 'Number of states: {}'.format(nr_states)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now add $(state, action)$ pairs to $Q$ and request pairs we havenâ€™t set yet (which get initialised randomly)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q(0, \"left\") = 0.5\n",
      "Q(1, \"up\") = 1.86257030614\n",
      "{0: {'down': 1.2, 'left': 0.5}, 1: {'up': 1.8625703061395276}}\n"
     ]
    }
   ],
   "source": [
    "q = QValues()\n",
    "q.set_value(0, 'down', 1.2)\n",
    "q.set_value(0, 'left', 0.5)\n",
    "print('Q(0, \"left\") = {}'.format(q.get_value(0, 'left')))\n",
    "print('Q(1, \"up\") = {}'.format(q.get_value(1, 'up')))\n",
    "print(q.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also request the action with the highest q value in a given state $s$ for a list of actions. For the Monte Carlo approch we can also request the action with the highest q value with an $\\epsilon$ chance of getting a random action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max action in state 0: down\n",
      "epsilon_greedy in state 0: left\n",
      "epsilon_greedy in state 0: down\n",
      "epsilon_greedy in state 0: down\n",
      "epsilon_greedy in state 0: left\n",
      "epsilon_greedy in state 0: left\n"
     ]
    }
   ],
   "source": [
    "print('Max action in state 0: {}'.format(q.max_action(0, ['down', 'left'])))\n",
    "for _ in range(5):\n",
    "    print('epsilon_greedy in state 0: {}'.format(q.epsilon_greedy(0, ['down', 'left'], epsilon=1.)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of an MDP we model the world as an environment. It holds the current state (e.g. coordinates of a robot) and defines the allowed actions in the current state (e.g. according to the current coordinates \"drive left\" or \"drive ahead\"). It allows the execution of an action and returns the next state and the reward for the action. Compare this definition to the definition in the lecture slides (page 9)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Environment(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        '''Initialise  environment.'''\n",
    "        pass\n",
    "    \n",
    "    def reset(self):\n",
    "        '''Reset environment to initial state.'''\n",
    "        pass\n",
    "   \n",
    "    def get_state(self):\n",
    "        '''Return current state.'''\n",
    "        pass\n",
    "    \n",
    "    def get_actions(self):\n",
    "        '''Return all allowed actions as list (might be empty).'''\n",
    "        pass\n",
    "\n",
    "    def exceute(self, action):\n",
    "        '''Set internal state to the new state according to action and return (new_state, reward) tuple.'''\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code is just a helper function to display a grid world example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def draw_gridworld(env, path=[]):\n",
    "    plt.plot()\n",
    "    plt.xlim(-1, 2)\n",
    "    plt.ylim(3, -1)\n",
    "    plt.grid()\n",
    "    plt.xticks([0,1])\n",
    "    plt.yticks([0,1,2])\n",
    "    plt.gca().set_xticklabels([])\n",
    "    plt.gca().set_yticklabels([])\n",
    "\n",
    "    plt.annotate('goal ' + str(env.goal), (env.goal[0]+0.25, env.goal[1]+0.5))\n",
    "    plt.annotate('start' + str(env.start), (env.start[0]+0.25, env.start[1]+0.5))        \n",
    "    for (x1, y1), (x2, y2) in env.walls:\n",
    "        if x1 == x2:\n",
    "            plt.plot([x1, x1+1],[max(y1,y2), max(y1,y2)], c='k')\n",
    "        else:\n",
    "            plt.plot([max(x1,x2), max(x1,x2)], [min(y1, y2), min(y1, y2)+1], c='k')\n",
    "    for (x,y), action in path:\n",
    "        plt.scatter(x+0.5, y+0.5, s=100, c='blue', marker={'left': '<', 'right':'>','up':'^', 'down':'v'}[action])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets define a grid word example as environment. We have a start, goal and current position. Each position is given in (x,y)-coordinates. The get_actions method allows 'left', 'right', 'up' and 'down' in any position except the final one. The reward for each action is -1. See how we can generate and infinit state space by just walking endless in one direction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class GridWorld(Environment):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.start = (0, 0)\n",
    "        self.goal = (0, 1)\n",
    "        self.position = (0, 0)\n",
    "        self.walls = []\n",
    "    \n",
    "    def reset(self):\n",
    "        self.position = self.start\n",
    "\n",
    "    def get_state(self):\n",
    "        return self.position\n",
    "    \n",
    "    def get_actions(self):\n",
    "        return [] if self.position == self.goal else ['left', 'right', 'up', 'down']\n",
    "        \n",
    "    def exceute(self, action):\n",
    "        position = self.get_state()\n",
    "        move = {'left':(-1,0), 'right':(1,0), 'up':(0,-1), 'down':(0,1)}[action]\n",
    "        self.position = (position[0] + move[0], position[1] + move[1])\n",
    "\n",
    "        reward = -1.\n",
    "        return self.position, reward\n",
    "    \n",
    "    def draw(self, path=[]):\n",
    "        draw_gridworld(self, path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For visualisation we display only the states around the start and goal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWQAAADuCAYAAAAOR30qAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fedec296b10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "GridWorld().draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code shows how $Q$-learning can be implemented. Our implementation will use the environment to read the current state, get the possible actions and get the reward after choosing an action.\n",
    "\n",
    "#### Q-learning Algorithm:\n",
    "\n",
    "Initialize $Q(s,a)$ arbitrarily<br>\n",
    "Repeat (for each episode):<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;Initialize $s$<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;Repeat (for each step of episode):<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Choose $a$ from $s$ using policy derived from $Q$ by $\\epsilon$-greedy<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Take action $a$, observe $r, s'$<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$Q(s,a) \\leftarrow Q(s,a) + \\alpha [r+ \\gamma $max$_{a'}Q(s',a')-Q(s,a)]$<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$s \\leftarrow s'$<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;until $s$ terminal<br>\n",
    "Output $Q$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def q_learning(env, q=None, nr_episodes=100, nr_steps=100, epsilon=0.1, alpha=0.1, gamma=0.98):\n",
    "    if not q:\n",
    "        q = QValues()\n",
    "\n",
    "    for _ in range(nr_episodes):\n",
    "        env.reset()\n",
    "        for _ in range(nr_steps):\n",
    "            state = env.get_state()\n",
    "            actions = env.get_actions()\n",
    "            if not actions:\n",
    "                break # final state reached\n",
    "\n",
    "            action = q.epsilon_greedy(state, actions, epsilon)\n",
    "            q_old = q.get_value(state, action)\n",
    "            \n",
    "            _, reward = env.exceute(action) #take action\n",
    "            next_state = env.get_state()\n",
    "            next_max_action = q.max_action(next_state, env.get_actions())\n",
    "            q_next = q.get_value(next_state, next_max_action)\n",
    "            \n",
    "            q_new = q_old + alpha * (reward + gamma * q_next - q_old)\n",
    "            q.set_value(state, action, q_new)\n",
    "    return q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We can now use the algorithm to derive $Q$ for the grid word example. From $Q$ we can get an optimal path by always choosing the actions with the highest expected reward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 0) down -> (0, 1)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWQAAADuCAYAAAAOR30qAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrsHxV7UAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fee067244d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "env = GridWorld()\n",
    "q = q_learning(env, nr_episodes=100, nr_steps=100)\n",
    "\n",
    "env.reset()\n",
    "best_path = []\n",
    "for _ in range (10):\n",
    "    state = env.get_state()\n",
    "    actions = env.get_actions()\n",
    "    if not actions:\n",
    "        break # final state reached\n",
    "\n",
    "    action = q.max_action(state, actions, learning=False)\n",
    "    next_state, _ = env.exceute(action)\n",
    "    print('{} {} -> {}'.format(state, action, next_state))\n",
    "\n",
    "    best_path.append((state, action))\n",
    "    \n",
    "env.draw(best_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected it was learned that the best state-action value from the initial state is down to the final state. Notice that during learning much more states where visited due to $\\epsilon$-greedy policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of states: 353\n"
     ]
    }
   ],
   "source": [
    "print(q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise\n",
    "\n",
    "Lets create a new grid world environment by defining two walls (one between start and goal and one between goal and the field right to goal). Rewrite the execute method in such way that a an action that would cause a movement through a wall results in saying in the current position (state). The reward should be -1 in any case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class GridWorld2(GridWorld):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.start = (0, 0)\n",
    "        self.goal = (0, 1)\n",
    "        self.position = (0, 0)\n",
    "        self.walls = [((0, 0), (0, 1)), # wall from (0,0) to (0,1)\n",
    "                      ((0, 1), (1, 1))] # wall from (0,1) to (1,1)\n",
    "            \n",
    "    def exceute(self, action):\n",
    "        position = self.get_state()\n",
    "        if (position == (0, 0) and action == 'down') or (position == (1, 1) and action == 'left'):\n",
    "            return position, -1\n",
    "        else:\n",
    "            move = {'left':(-1,0), 'right':(1,0), 'up':(0,-1), 'down':(0,1)}[action]\n",
    "            self.position = (position[0] + move[0], position[1] + move[1])\n",
    "\n",
    "            reward = -1.\n",
    "            return self.position, reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the code was implemented correct the optimal path found should be:\n",
    "\n",
    "Position,  Action -> Next Position <br>\n",
    "(0, 0),    left   -> (-1, 0)<br>\n",
    "(-1, 0),   down   -> (-1, 1)<br>\n",
    "(-1, 1),   right  -> (0, 1)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 0) left -> (-1, 0)\n",
      "(-1, 0) down -> (-1, 1)\n",
      "(-1, 1) right -> (0, 1)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWQAAADuCAYAAAAOR30qAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nACMIMgAYQZABwAiCDABGEGQAMOL/AYLx9NJbNdPCAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fedea178850>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "env = GridWorld2()\n",
    "q = q_learning(env, nr_episodes=500, nr_steps=100, epsilon=0.1, alpha=0.1, gamma=0.98)\n",
    "\n",
    "env.reset()\n",
    "best_path = []\n",
    "for _ in range (10):\n",
    "    state = env.get_state()\n",
    "    actions = env.get_actions()\n",
    "    if not actions:\n",
    "        break # final state reached\n",
    "\n",
    "    action = q.max_action(state, actions, learning=False)\n",
    "    next_state, _ = env.exceute(action)\n",
    "    print('{} {} -> {}'.format(state, action, next_state))\n",
    "\n",
    "    best_path.append((state, action))\n",
    "    \n",
    "env.draw(best_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question\n",
    "\n",
    "When lowering the nr_episodes (e.g. 50) in the above example the found path may not be optimal or leading to the final state at all. What are the reason for this behaviour?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer\n",
    "\n",
    "- $Q$-Learning does not guarantee to result in $Q^*$\n",
    "- too few episodes or steps might result in never reaching the final state\n",
    "- find good balance between exploration and exploitation\n",
    "- init values for $Q$ should be chosen with respoect to the rewards (here random negative instead of positive)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of implementing $Q$ as lookup table, one can use a neuronal network. Think about possible implementations and how the training would change. One possible solution will be presented in the exercise."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
