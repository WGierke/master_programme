{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 11: Topic modeling with latent Dirichlet allocation (LDA) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this practical exercise, we will use latent Dirichlet allocation (LDA) to discover the topics prevalent in a document collection, and assign topics to the documents. As discussed in the lecture, an LDA model describes each topic in terms of a distribution over words, and each document as a distribution over topics. The problem setting is unsupervised in the sense that only the text in the documents is observed, and all other variables are latent and need to be inferred by the model.\n",
    "\n",
    "We will study the <a href=\"https://archive.ics.uci.edu/ml/datasets/Reuters-21578+Text+Categorization+Collection\"> Reuters-21578 </a> document collection, which contains news wire articles that appeared on Reuters in 1987.\n",
    "\n",
    "First we download and load the Reuters-21578 dataset using the *Natural Language Toolkit* (*NLTK*), which is a platform for handling natural language in Python. It provides access to several text corpora and functions for handling text data. We first download the Reuters-21578 data and import it into the list $articles$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package reuters to /home/madness/nltk_data...\n",
      "[nltk_data]   Package reuters is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.corpus import reuters\n",
    "nltk.download('reuters')\n",
    "articles=[]\n",
    "for doc_id in reuters.fileids():\n",
    "    articles.append(reuters.raw(doc_id))\n",
    "articles = articles[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variable $articles$ now contains the first 1000 documents in the form of strings.\n",
    "\n",
    "As discussed in the lecture, LDA is a bag-of-words model, meaning that the probabilistic process for a document does not take into account the order in which the words appear in a document. This means that for performing inference with the LDA model, it suffices to know which words appear in a document and how often each word appears. Therefore we will pass the document collection as a document word frequency matrix. The document word frequency matrix is a 2D array where rows represent documents and columns represent words and each cell counts how many times the specific word appears in a given document.\n",
    "\n",
    "The following code constructs a document word matrix $tf$, pruning the vocabulary of words such that it only contains words  that:\n",
    "\n",
    "* have latin characters and are of length 3 or more characters (token_pattern='[a-zA-Z]{3,}'),\n",
    "\n",
    "* are not english stop words, that is, frequent but uninformative words such as \"the\" or \"a\" (stop_words='english')\n",
    "\n",
    "* occur in at least 0.2% of all documents and at most 95% of all documents (max_df=0.95, min_df=0.002)\n",
    "\n",
    "and out of these using the 2000 most frequent words (max_features=2000)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "tf = CountVectorizer( token_pattern='[a-zA-Z]{3,}',max_df=0.95, min_df=0.002,max_features=2000,stop_words='english')\n",
    "articles_words = tf.fit_transform(articles)\n",
    "word_index = tf.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 1:  \n",
    "Using sklearn.decomposition.LatentDirichletAllocation, train a LDA model with $K=20$ topics on the document collection using the document word frequency matrix *articles_words*. You can find the function documentation <a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html\"> here</a>. You can leave the Dirichlet prior parameters $\\alpha$ and $\\eta$ at their default values of $1/K$. Hint: use the argument enabling multicore processing to increase the speed of inference. Use the argument *random_state=0* to set the random seed of the inference algorithm in order to get reproducible results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/madness/venv3/lib/python3.5/site-packages/sklearn/decomposition/online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LatentDirichletAllocation(batch_size=128, doc_topic_prior=None,\n",
       "             evaluate_every=-1, learning_decay=0.7, learning_method=None,\n",
       "             learning_offset=10.0, max_doc_update_iter=100, max_iter=10,\n",
       "             mean_change_tol=0.001, n_components=20, n_jobs=-1,\n",
       "             n_topics=None, perp_tol=0.1, random_state=0,\n",
       "             topic_word_prior=None, total_samples=1000000.0, verbose=0)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "k = 20\n",
    "lda = LatentDirichletAllocation(n_components=k, random_state=0, n_jobs=-1)\n",
    "lda.fit(articles_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 2:  \n",
    "Describe each topic by listing the 10 most probable words in that topic. Please check LatentDirichletAllocation function documentation on how to obtain the word distributions per topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0: tonnes sugar said traders tender export price week sources ecus\n",
      "Topic #1: mln cts net shr dlrs loss qtr profit revs note\n",
      "Topic #2: japan trade said japanese exports officials imports washington ministry products\n",
      "Topic #3: revlon issue macandrews apr bonus gordex jardine matheson cape spencer\n",
      "Topic #4: oil said gas field crude barrels exxon exploration reserves petroleum\n",
      "Topic #5: yen half company dividend paid current interim caused motor slow\n",
      "Topic #6: pct said mln rate billion market bank week days banks\n",
      "Topic #7: rtz norway unilever chesebrough performance banking wholly subsidiaries buyer bankers\n",
      "Topic #8: said year pct dlrs mln quarter prices sales company billion\n",
      "Topic #9: march unemployment workforce government beverage people swedish price faygo bureau\n",
      "Topic #10: said dlrs dollar billion analysts fed markets deficit trade market\n",
      "Topic #11: dlrs loans mln non loan brazil income bank quarter trust\n",
      "Topic #12: bank stg yen mln dollar dealers market bills dollars said\n",
      "Topic #13: said wheat coffee department tonnes export delivery april futures price\n",
      "Topic #14: said shares company stock share dlrs mln corp offer pct\n",
      "Topic #15: gas energy natural atlantic sulphur pipeline williams rjr showboat nabisco\n",
      "Topic #16: nil billion prev south canada surplus named lire unnamed total\n",
      "Topic #17: mln tonnes stocks production exports grain month crop imports wheat\n",
      "Topic #18: said dollar meeting paris monetary west agreement economic countries group\n",
      "Topic #19: standard bell mobil talks conrac negotiating producer comment details parties\n"
     ]
    }
   ],
   "source": [
    "n_top_words = 10\n",
    "\n",
    "# components_[i, j] can be viewed as count that represents the number of times word j was assigned to topic i\n",
    "# np.argpartition returns the arguments of the k biggest or smallest elements in no order\n",
    "for topic_idx, topic in enumerate(lda.components_):\n",
    "        message = \"Topic #%d: \" % topic_idx\n",
    "        indices = np.argpartition(topic, -n_top_words)[-n_top_words:]\n",
    "        indices = indices[np.argsort(-topic[indices])]\n",
    "        message += \" \".join([word_index[i] for i in indices])\n",
    "        print(message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 3: \n",
    "From the inferred topic distributions, we can define the topic distance between two documents $d_1$, $d_2$ to be the Kullback-Leibler divergence between their topic distributions. Let $\\theta_{d_j}$ for $j \\in \\{1,2\\}$ be the parameter vector of the categorical topic distributions for documents $d_1$ and $d_2$, and let $\\theta_{d_j,i}$ denote their $i$-th component, that is, the probability for topic $i$. \n",
    "\n",
    "The Kullback-Leibler divergence is defined by\n",
    "\n",
    "$$KL(d_1,d_2) = \\sum_i{\\theta_{d_1,i}}\\log{\\frac{\\theta_{d_1,i}}{\\theta_{d_2,i}}}$$\n",
    "\n",
    "Implement a function *get_similar(doc_id,doc_topic_distribution)* that takes an integer *doc_id* representing the document index and a matrix that gives the distribution over topics for each document. The function should return a list that contains the indices of all documents in the collection ordered by their topic distance to *doc_id*. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [03:10<00:00,  5.24it/s]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "doc_topic_distribution = np.zeros((articles_words.shape[1], 20))\n",
    "for doc_index in tqdm(range(articles_words.shape[0])):\n",
    "    doc_topic_distribution[doc_index, :] = lda.transform(articles_words[doc_index, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[728 802 698 521 803 708 187 761 325 202]\n",
      "[728 802 698 521 803 708 187 761 325 202]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/madness/venv3/lib/python3.5/site-packages/scipy/stats/_distn_infrastructure.py:2512: RuntimeWarning: invalid value encountered in true_divide\n",
      "  qk = 1.0*qk / np.sum(qk, axis=0)\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import entropy\n",
    "\n",
    "def get_similar(doc_id, doc_topic_distribution, top=None):\n",
    "    doc_scores = np.ones(doc_topic_distribution.shape[0])\n",
    "    for doc_index, doc in enumerate(doc_topic_distribution):\n",
    "        doc_scores[doc_index] = entropy(doc_topic_distribution[doc_id], doc)\n",
    "    # Use argpartition for faster sorting\n",
    "    # Omit the first element as it is the document itself\n",
    "    if top is None:\n",
    "        return np.argsort(doc_scores)[1:]\n",
    "    else:\n",
    "        indices = np.argpartition(doc_scores, range(top+1))[1:top+1]\n",
    "        return indices[np.argsort(doc_scores[indices])]\n",
    "\n",
    "print(get_similar(0, doc_topic_distribution)[:10])\n",
    "print(get_similar(0, doc_topic_distribution, top=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 4:  \n",
    "\n",
    "Get the 10  documents that are most similar to the document with index 1 according to topic distance, and inspect manually whether they are all indeed covering similar content as this document.\n",
    "\n",
    "To get the distribution of topics over documents, use the LatentDirichletAllocation transform function. It takes the document word matrix and returns an un-normalized document topic distribution, so you have to normalize the matrix before using it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BRAZIL SOYBEAN YIELDS SEEN AVERAGE - USDA REPORT\n",
      "  Based on field travel in the\n",
      "  Brazilian state of\n",
      "ROTTERDAM GRAIN HANDLER SAYS PORT BALANCE ROSE\n",
      "  Graan Elevator Mij, GEM, said its\n",
      "  balance in port\n",
      "FAO SEES LOWER GLOBAL WHEAT, GRAIN OUTPUT IN 1987\n",
      "  The U.N. Food and Agriculture Organisation\n",
      "  (FA\n",
      "BRAZIL COTTON CROP LOWER -- USDA REPORT\n",
      "  Brazil's 1986/87 cotton crop estimate\n",
      "  has been reduced t\n",
      "SOVIET UNION TO IMPORT MORE GRAIN IN 86/87-USDA\n",
      "  The U.S. Agriculture Department\n",
      "  increased its es\n",
      "BRAZIL GRAIN HARVEST FACES STORAGE PROBLEMS\n",
      "  Storage problems with Brazil's record\n",
      "  grain crop are\n",
      "DRY SPELL IN PHILIPPINES DAMAGES AGRICULTURE CROPS\n",
      "  A prolonged dry spell has damaged\n",
      "  111,350 hec\n",
      "WEATHER HURTS ITALIAN ORANGES - USDA REPORT\n",
      "  Unfavorable weather conditions during\n",
      "  the second wee\n",
      "INDIA OILSEED OUTPUT FORECAST TO RISE\n",
      "  India's oilseed output is expected to\n",
      "  rise to 12.25 mln to\n",
      "SUBROTO SAYS INDONESIA SUPPORTS TIN PACT EXTENSION\n",
      "  Mines and Energy Minister Subroto\n",
      "  confirmed I\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/madness/venv3/lib/python3.5/site-packages/scipy/stats/_distn_infrastructure.py:2512: RuntimeWarning: invalid value encountered in true_divide\n",
      "  qk = 1.0*qk / np.sum(qk, axis=0)\n"
     ]
    }
   ],
   "source": [
    "similar_doc_indices = get_similar(1, doc_topic_distribution, top=10)\n",
    "for doc_index in similar_doc_indices:\n",
    "    print(articles[doc_index][:100]) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
