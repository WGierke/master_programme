{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 10: Approximate Inference in Graphical Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this practical exercise, we implement the Gibbs sampling and logic sampling approximate inference algorithms. \n",
    "\n",
    "We study the graphical model over the six binary variables $x_1,x_2,x_3,x_4,x_5,x_6$ given in Exercise 1 of the last set of exercises (see last exercise sheet). \n",
    "\n",
    "We first define the graph structure by a variable *parents* that lists the parent nodes of each node. Because list indices start at zero, we use indices $0,...,5$ to refer to the nodes $x_1,x_2,x_3,x_4,x_5,x_6$. We also define a variable *tables* that represents the conditional probability tables for all nodes as defined in the exercise sheet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parents = [[],[0],[1],[2],[0,3],[0,4]]\n",
    "tables = [[0.5],[0.6,0.3],[0.7,0.4],[0.2,0.5],[0.3,0.6,0.5,0.1],[0.5,0.1,0.3,0.4]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As discussed in the lecture, the graphical model defines a joint probability distribution \n",
    "\n",
    "$$p(x_1,...,x_6) = \\prod_{i=1}^6 p(x_i \\mid pa(x_i))$$.\n",
    "\n",
    "The function *joint_probability* computes this joint probability for a particular joint state of the six random variables. Here, the joint state is given by a list of six values $x_i \\in \\{0,1\\}$. The function is based on the function *probability_factor*, which computes the probability $p(x_i \\mid pa(x_i))$ for a certain node given the state of its parents. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def joint_probability(joint_state):\n",
    "    joint_state = np.array(joint_state)\n",
    "    p = 1;\n",
    "    for i in range(0,len(joint_state)): \n",
    "        p = p*probability_factor(joint_state[i],joint_state[parents[i]],tables[i])\n",
    "    return p\n",
    "\n",
    "def probability_factor(node_state,parent_state,probability_table):\n",
    "    index = 0\n",
    "    for j in range(0,len(parent_state)):\n",
    "        index = index + 2**j*int(parent_state[j])\n",
    "    if node_state:\n",
    "        return probability_table[index]\n",
    "    else:\n",
    "        return 1-probability_table[index] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, the following call computes the probability $p(x_1=0,x_2=1,x_3=0,x_4=1,x_5=0,x_6=1)$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability of joint state [0,1,0,1,0,1]: 0.009000.\n"
     ]
    }
   ],
   "source": [
    "print(\"Probability of joint state [0,1,0,1,0,1]: %f.\" % joint_probability([0,1,0,1,0,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also supply a function *node_conditional_probability*, which computes the conditional probability $p(x_i = 1 \\mid x_1,...,x_{i-1},x_{i+1},...,x_n)$ of $x_i$ being true given a joint state of all other nodes. The argument *i* is the index of the variable for which to compute the conditional, note that again the index range is $0,...,5$ for the variables $x_1,...,x_6$. The argument *joint_state* is a list of states for all nodes, the element *joint_state[i]* is ignored. As discussed in the lecture, this probability can be computed efficiently by straightforward application of the definition of conditional probability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def node_conditional_probability(i,joint_state):\n",
    "    p0 = joint_probability(np.concatenate((joint_state[:i],[0],joint_state[i+1:]))) \n",
    "    p1 = joint_probability(np.concatenate((joint_state[:i],[1],joint_state[i+1:])))\n",
    "    return p1/(p0+p1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, the following call computes the probability $p(x_3 = 1 \\mid x_1=0,x_2=1,x_4=1,x_5=0,x_6=1)$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conditional probability p(x3=1|x1=0,x2=1,x4=1,x5=0,x6=1): 0.625000\n"
     ]
    }
   ],
   "source": [
    "print(\"Conditional probability p(x3=1|x1=0,x2=1,x4=1,x5=0,x6=1): %f\" % node_conditional_probability(2,[0,1,0,1,0,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1:\n",
    "\n",
    "#### 1.1: \n",
    "\n",
    "Implement a function *gibbs_sampling(evidence,K)* that generates K samples using the Gibbs sampling algorithm. \n",
    "The argument *evidence* is a list $[e_1,e_2,e_3,e_4,e_5,e_6]$ of integer values, where a value of $e_i = 0$ indicates \n",
    "evidence $x_i=0$, $e_i = 1$ indicates $x_i=1$, and $e_i = -1$ indicates no evidence on the variable $x_i$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2:\n",
    "\n",
    "Use the implemented algorithm to approximately solve the inference problem $p(x_3 \\mid x_1=0, x_4=1)$ that we already solved exactly using the message passing algorithm in the last exercise session. That is, compute a set of samples by\n",
    "\n",
    "*samples = gibbs_sampling(evidence,K)*\n",
    "\n",
    "and approximate the conditional distribution over the query variable $x_3$ by computing the fraction of times the state of $x_3$ was 1 over all samples.\n",
    "\n",
    "Reminder: the solution from message passing was $p(x_3 = 1 \\mid x_1=0, x_4=1) \\approx 0.73$. Your algorithm should converge to this solution. Run the algorithm for $K = 100,1000,10000,100000$ samples, and note how the approximation error changes as a function of K. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: \n",
    "\n",
    "Also implement logic sampling, and use it to approximately solve the same inference problem. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
