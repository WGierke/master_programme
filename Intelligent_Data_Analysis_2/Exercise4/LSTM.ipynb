{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: LSTM for Sequence Classification on the IMDB dataset\n",
    "\n",
    "In a first exercise we want to build a model that can classify movie reviews as positive or negative based on their sentiment. To do so we train a model using the IMDB dataset. \n",
    "\n",
    "The IMDB dataset consists of 25,000 movies reviews from IMDB, labeled by sentiment (positive/negative). Reviews have been preprocessed, and each review is encoded as a sequence of word indexes (integers). For convenience, words are indexed by overall frequency in the dataset, so that for instance the integer \"3\" encodes the 3rd most frequent word in the data. This allows for quick filtering operations such as: \"only consider the top 10,000 most common words, but eliminate the top 20 most common words\". As a convention, \"0\" does not stand for a specific word, but instead is used to encode any unknown word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import numpy\n",
    "# fix random seed so every student get the same results\n",
    "numpy.random.seed(7)\n",
    "\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to load the IMDB dataset. We are constraining the dataset to the top 5,000 words. We also split the dataset into train (50%) and test (50%) sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://s3.amazonaws.com/text-datasets/imdb.npz\n",
      "17375232/17464789 [============================>.] - ETA: 0s(25000,) (25000,) (25000,) (25000,)\n"
     ]
    }
   ],
   "source": [
    "# load the dataset but only keep the top n words, zero the rest\n",
    "from keras.datasets import imdb\n",
    "top_words = 5000\n",
    "if 'X_train' not in locals():\n",
    "    (X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=top_words)\n",
    "print(X_train.shape,X_test.shape,y_train.shape,y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Next, we need to truncate and pad the input sequences so that they are all the same length for modeling. In this tutorial we use a maximum length of 100.\n",
    "\n",
    "<b>Exercise 1</b>: \n",
    "\n",
    "To easily use an LSTM we need to truncate and pad the input sequences so that they are all of the same length. Create the training data by using the first 5.000 examples of X_train as training data and the first 5.000 examples of X_test as validation data. Use maxlen=100 to truncate all sequences to the same length.\n",
    "\n",
    "The model will learn the zero values carry no information so indeed the sequences are not the same length in terms of content, but same length vectors is required to perform the computation in Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "max_review_length = 100\n",
    "AMOUNT = 5000\n",
    "\n",
    "X_train = pad_sequences(X_train[:AMOUNT], maxlen=max_review_length)\n",
    "X_test = pad_sequences(X_test[:AMOUNT], maxlen=max_review_length)\n",
    "y_train = y_train[:AMOUNT]\n",
    "y_test = y_test[:AMOUNT]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if evereything is correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing successful.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    assert X_train.shape == (5000, 100)\n",
    "    assert X_test.shape == (5000, 100)\n",
    "    print(\"Testing successful.\")\n",
    "except:\n",
    "    print(\"Tests failed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now define, compile and fit our LSTM model.\n",
    "\n",
    "The first layer is the embedded layer that uses 32 length vectors to represent\n",
    " each word. The next layer is the LSTM layer with 5 memory units (smart neurons). \n",
    " Finally, because this is a classification problem we use a dense output \n",
    " layer with a single neuron and a sigmoid activation function to make 0 or 1 \n",
    " predictions for the two classes (good or bad) in the problem.\n",
    "\n",
    "Because it is a binary classification problem, log loss is used as the loss \n",
    "function (binary_crossentropy in Keras). The efficient ADAM optimization \n",
    "algorithm is used to do the gradient descent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 100, 32)           160000    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 5)                 760       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 6         \n",
      "=================================================================\n",
      "Total params: 160,766\n",
      "Trainable params: 160,766\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "embedding_vecor_length = 32\n",
    "model = Sequential()\n",
    "model.add(Embedding(top_words, embedding_vecor_length, input_length=max_review_length))\n",
    "model.add(LSTM(5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Exercise 2:</b>\n",
    "\n",
    "Train a model for only 10 epochs and evaluate the model on the test data. Use a batch size of 64 reviews to update the weights in the LSTM net. Use X_test and y_test to validate the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5000 samples, validate on 5000 samples\n",
      "Epoch 1/10\n",
      "5000/5000 [==============================] - 8s - loss: 0.6843 - acc: 0.6094 - val_loss: 0.6586 - val_acc: 0.6694\n",
      "Epoch 2/10\n",
      "5000/5000 [==============================] - 8s - loss: 0.5332 - acc: 0.8096 - val_loss: 0.4830 - val_acc: 0.8138\n",
      "Epoch 3/10\n",
      "5000/5000 [==============================] - 8s - loss: 0.3661 - acc: 0.8940 - val_loss: 0.4364 - val_acc: 0.8204\n",
      "Epoch 4/10\n",
      "5000/5000 [==============================] - 8s - loss: 0.2763 - acc: 0.9268 - val_loss: 0.4446 - val_acc: 0.8000\n",
      "Epoch 5/10\n",
      "5000/5000 [==============================] - 8s - loss: 0.2196 - acc: 0.9468 - val_loss: 0.4408 - val_acc: 0.8082\n",
      "Epoch 6/10\n",
      "5000/5000 [==============================] - 8s - loss: 0.1785 - acc: 0.9572 - val_loss: 0.4605 - val_acc: 0.8054\n",
      "Epoch 7/10\n",
      "5000/5000 [==============================] - 8s - loss: 0.1454 - acc: 0.9684 - val_loss: 0.4868 - val_acc: 0.8072\n",
      "Epoch 8/10\n",
      "5000/5000 [==============================] - 8s - loss: 0.1246 - acc: 0.9740 - val_loss: 0.4959 - val_acc: 0.8010\n",
      "Epoch 9/10\n",
      "5000/5000 [==============================] - 8s - loss: 0.1067 - acc: 0.9790 - val_loss: 0.5364 - val_acc: 0.7912\n",
      "Epoch 10/10\n",
      "5000/5000 [==============================] - 8s - loss: 0.0887 - acc: 0.9834 - val_loss: 0.5543 - val_acc: 0.7928\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "epochs = 10\n",
    "history = model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we fitted the model, we estimate the performance of the model on unseen reviews.\n",
    "\n",
    "<b>Exercise 3:</b>\n",
    "\n",
    "Evaluate the performance on the test set consisting of X_test and y_test and report the accuracy and use a ROC curve to show the performance of the model (Hint: use the 'evaluate' function to test the performance). What conclusions can you draw from the ROC curve?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4864/5000 [============================>.] - ETA: 0s"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.55425304374694828, 0.79279999999999995]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4928/5000 [============================>.] - ETA: 0s"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/wEhR2xW0ie2kAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fdd8f965cf8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "y_pred = model.predict_proba(X_test)\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "plt.plot(fpr, tpr, label='ROC AUC = %0.2f)' % (roc_auc))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The AUV score tells us that the probability of predicting a true positive is 87%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that this simple LSTM achieves good results on the IMDB problem. Importantly, this is a template that you can use to apply LSTM networks to your own sequence classification problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Generate sentences using LSTMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part of the lab we are using a pretrained model (you can train the model on your own, but this will take some time). This model was trained to predict the next character given a sequence of 100 previous characters. This model can be used to generate new sentences/phrases. Your task is to use this model and generate sentences with it. The model was pretrained on a book called \"wonderland\".\n",
    "\n",
    "First of all we want to load the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load ascii text and covert to lowercase\n",
    "filename = \"wonderland.txt\"\n",
    "raw_text = open(filename, encoding=\"utf8\").read()\n",
    "raw_text = raw_text.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the book is loaded, we must prepare the data for modeling by the neural network. We cannot model the characters directly, instead we must convert the characters to integers.\n",
    "\n",
    "We can do this easily by first creating a set of all of the distinct characters in the book, then creating a map of each character to a unique integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create mapping of unique chars to integers\n",
    "chars = sorted(list(set(raw_text)))\n",
    "char_to_int = dict((c, i) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, the list of unique unicode sorted lowercase characters in the book is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', ' ', '!', '#', '$', '%', '(', ')', '*', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?', '@', '[', ']', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '‘', '’', '“', '”', '\\ufeff']\n"
     ]
    }
   ],
   "source": [
    "print(chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that there may be some characters that we could remove to further clean up the dataset that will reduce the vocabulary and may improve the modeling process. In this lab we skip this process.\n",
    "\n",
    "Now that the book has been loaded and the mapping prepared, we can summarize the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Characters: 163817\n",
      "Total Vocab: 61\n"
     ]
    }
   ],
   "source": [
    "n_chars = len(raw_text)\n",
    "n_vocab = len(chars)\n",
    "print(\"Total Characters: \" + str(n_chars))\n",
    "print(\"Total Vocab: \" + str(n_vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the book has just under 160,000 characters and that when converted to lowercase that there are only 61 distinct characters in the vocabulary for the network to learn.\n",
    "\n",
    "We now need to define the training data for the network. There is a lot of flexibility in how you choose to break up the text and expose it to the network during training.\n",
    "\n",
    "In this tutorial (as explained above) we will split the book text up into subsequences with a fixed length of 100 characters.\n",
    "\n",
    "Each training pattern of the network is comprised of 100 time steps of one character (X) followed by one character output (y). When creating these sequences, we slide this window along the whole book one character at a time, allowing each character a chance to be learned from the 100 characters that preceded it.\n",
    "\n",
    "For example, if the sequence length is 5 (for simplicity) then the first two training patterns would be as follows:\n",
    "\n",
    "CHAPT -> E, HAPTE -> R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we split up the book into these sequences, we convert the characters to integers using our lookup table we prepared earlier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Exercise 4:</b>\n",
    "\n",
    "Create all patterns. Create a list of sequences dataX that contains all windows of the book and a list of following characters dataY. So each entry in the list dataX contains of a vector of 100 integer values representing the characters that occur in the window. Each entry in the list dataY contains the following character for the associated window in dataX.\n",
    "\n",
    "Hint: Use the dictionary char_to_int to map the characters to integers. Look at the cell below to see an example entry of dataX and dataY."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if everything is correct:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# prepare the dataset of input to output pairs encoded as integers\n",
    "seq_length = 100\n",
    "dataX = []\n",
    "\n",
    "for i in range(len(raw_text)-seq_length):\n",
    "    string_window = raw_text[i:i+seq_length]\n",
    "    int_window = [char_to_int[s] for s in string_window]\n",
    "    dataX.append(int_window)\n",
    "\n",
    "dataY = [dataX[i+1][-1] for i in range(len(dataX)-1)]\n",
    "dataY.append(char_to_int[raw_text[-1]])\n",
    "assert len(dataX) == len(dataY), \"Both are not equally long: {} and {}\".format(len(dataX), len(dataY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing successful.\n"
     ]
    }
   ],
   "source": [
    "n_patterns = len(dataX)\n",
    "try:\n",
    "    assert n_patterns >= 163717, \"n_patterns is {}\".format(n_patterns)\n",
    "    assert dataX[0] == [60, 45, 47, 44, 39, 34, 32, 49, 1, 36, 50, 49, 34, 43, 31, 34, 47, 36, 57, 48, 1, 30, 41, 38, 32, 34, 57, 48, 1, 30, 33, 51, 34, 43, 49, 50, 47, 34, 48, 1, 38, 43, 1, 52, 44, 43, 33, 34, 47, 41, 30, 43, 33, 9, 1, 31, 54, 1, 41, 34, 52, 38, 48, 1, 32, 30, 47, 47, 44, 41, 41, 0, 0, 49, 37, 38, 48, 1, 34, 31, 44, 44, 40, 1, 38, 48, 1, 35, 44, 47, 1, 49, 37, 34, 1, 50, 48, 34, 1, 44], \"dataX[0] is wrong\"\n",
    "    assert dataY[0] == 35, \"dataY[0] is {}\".format(dataY[0])\n",
    "    print(\"Testing successful.\")\n",
    "except Exception as e:\n",
    "    print(\"Tests failed: {}\".format(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the code to this point shows us that when we split up the dataset into training data for the network to learn that we have just under 160,000 training patterns. \n",
    "\n",
    "Now that we have prepared our training data we need to transform it so that it is suitable for use with Keras.\n",
    "\n",
    "First we must transform the list of input sequences into the form [samples, features] expected by an LSTM network (if we would have more features per time step the dimension would be [samples, time_steps, features]).\n",
    "\n",
    "Finally, we need to convert the output patterns (single characters converted to integers) into a one hot encoding. This is so that we can configure the network to predict the probability of each of the 61 different characters in the vocabulary (an easier representation) rather than trying to force it to predict precisely the next character. Each y value is converted into a sparse vector with a length of 61, full of zeros except with a 1 in the column for the letter (integer) that the pattern represents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# reshape X to be [samples, time steps]\n",
    "X = numpy.reshape(dataX, (n_patterns, seq_length))\n",
    "# one hot encode the output variable\n",
    "from keras.utils import np_utils\n",
    "y = np_utils.to_categorical(dataY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can define our model. It consists of an embedding layer that embeds each character in a 32-dimensional feature space. The next layer is a layer with 256 LSTM units followed by a Dropout-Layer to reduce overfitting. The last layer is a Dense-Layer used to predict the probabilities for each of the 61 characters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 100, 32)           1952      \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 256)               295936    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 60)                15420     \n",
      "=================================================================\n",
      "Total params: 313,308\n",
      "Trainable params: 313,308\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "embedding_vecor_length = 32\n",
    "top_words = n_vocab\n",
    "model = Sequential()\n",
    "model.add(Embedding(top_words, embedding_vecor_length, input_length=X.shape[1]))\n",
    "model.add(LSTM(256))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now fit our model to the data. Here we use a modest number of 20 epochs and a large batch size of 128 patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define the checkpoint\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "filepath=\"weights-improvement-{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training this model really takes some time. So you can skip the learning step and use the pretrained model instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#hist = model.fit(X, y, epochs=20, batch_size=128,callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Excercise 5</b>:\n",
    "    \n",
    "Use the weights stored in the file  \"best_weights.hdf5\" (model with the best weights) and compile the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#hist = model.fit(X, y, epochs=20, batch_size=128,callbacks=callbacks_list)\n",
    "\n",
    "# load the network weights\n",
    "filename = \"best_weights.hdf5\"\n",
    "model.load_weights(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Exercise 6:</b>\n",
    "    \n",
    "Also, when preparing the mapping of unique characters to integers, we must also create a reverse mapping that we can use to convert the integers back to characters so that we can understand the predictions. Create a mapping from the integers to the characters as we did by defining the dictionary char_to_int."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# YOUR CODE GOES HERE\n",
    "int_to_char = {v: k for k, v in char_to_int.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Finally, we need to actually make predictions.\n",
    "\n",
    "The simplest way to use the Keras LSTM model to make predictions is to first start off with a seed sequence as input, generate the next character then update the seed sequence to add the generated character on the end and trim off the first character. This process is repeated for as long as we want to predict new characters (e.g. a sequence of 100 characters in length).\n",
    "\n",
    "We can pick a random input pattern as our seed sequence, then print generated characters as we generate them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Exercise 7:</b>\n",
    "    \n",
    "Use the pretrained (our your own trained) LSTM and predict the next 100 characters using a random sequence as starting point. How could you easily obtain different predictions for the next character?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed:\n",
      "eshire puss,’ she began, rather timidly, as she did not at all know\n",
      "whether it would like the name: \n",
      "prediction: \n",
      "‘it’s a founderation of the sea.\n",
      "\n",
      "                                                                  \n",
      "\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# Excercise\n",
    "# pick a random seed\n",
    "start = numpy.random.randint(0, len(dataX)-1)\n",
    "pattern = dataX[start]\n",
    "print(\"Seed:\")\n",
    "print(''.join([int_to_char[value] for value in pattern]))\n",
    "\n",
    "# generate characters\n",
    "for i in range(max_review_length):\n",
    "    x = numpy.reshape(pattern, (1, len(pattern)))\n",
    "    prediction = model.predict(x, verbose=0)\n",
    "    index = numpy.argmax(prediction)\n",
    "    result = int_to_char[index]\n",
    "    seq_in = [int_to_char[value] for value in pattern]\n",
    "    pattern.append(index)\n",
    "    pattern = pattern[1:len(pattern)]\n",
    "    \n",
    "print(\"prediction: \")\n",
    "print(\"\".join([int_to_char[i] for i in pattern]))\n",
    "print(\"\\nDone.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Exercise 8:</b>\n",
    "    \n",
    "Use a sentence with at least 100 characters created by your own and look at the next predicted 100 characters. How does the prediction look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction: \n",
      "\n",
      "\n",
      "‘i shall have to be off, and the moral of the most way i think?’ said alice.\n",
      "\n",
      "‘it was a little bes\n",
      "\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "sentence = \"This sentence contains exactly onehundred words. Coincidence? I think not! Follow the white rabbit..\".lower()\n",
    "assert len(sentence) == 100, len(sentence)\n",
    "pattern = [char_to_int[c] for c in sentence]\n",
    "\n",
    "for i in range(max_review_length):\n",
    "    x = numpy.reshape(pattern, (1, len(pattern)))\n",
    "    prediction = model.predict(x, verbose=0)\n",
    "    index = numpy.argmax(prediction)\n",
    "    result = int_to_char[index]\n",
    "    seq_in = [int_to_char[value] for value in pattern]\n",
    "    pattern.append(index)\n",
    "    pattern = pattern[1:len(pattern)]\n",
    "    \n",
    "print(\"prediction: \")\n",
    "print(\"\".join([int_to_char[i] for i in pattern]))\n",
    "print(\"\\nDone.\")"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "home_venv3",
   "language": "python",
   "name": "home_venv3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
