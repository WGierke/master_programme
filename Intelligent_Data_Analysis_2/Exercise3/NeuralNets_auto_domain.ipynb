{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Neural Nets: Auto Encoder and Domain Adaptation</b>\n",
    "\n",
    "In this lab you will learn how to use neural nets in a setting where we use a pretrained model (trained on a big data set) for a slightly different task as initialization (in this tutorial a neural net that can recognize the digits 0-7). Building a model on top of a pretrained model (training a neural net to recognize the digits 8 and 9) by only fine tuning the initial weights can give better results than training a neural net from scratch. This technique is known as domain adaptation. Furthermore we will look at autoencoders and how they can be used for denoising data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from keras.datasets import mnist\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# nn\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential, Model, Input\n",
    "from keras.layers.core import Dense, Dropout, Flatten, Reshape, Activation\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D, UpSampling2D\n",
    "from keras.utils import np_utils\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# image manipulation\n",
    "import cv2\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>1. Part: Domain Adaptation</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, we load the data set (handwritten digit data set) and rescale all data points as in the tutorial before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load digit dataset with training and test images\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "# rescale the data\n",
    "x_train = x_train / 255.\n",
    "x_test = x_test / 255.\n",
    "# dimension\n",
    "img_rows, img_cols = x_train[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# transform data set\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
    "    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
    "    input_shape = (1, img_rows, img_cols)\n",
    "else:\n",
    "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
    "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
    "    input_shape = (img_rows, img_cols, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create two slightly different task by dividing the data into two data sets. The first data set contains all images for the digits ranging from 0 to 7. The second data set contains images with images of handwritten 8's and 9's. For the second data set we only use the first 10 instances in the training part of the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get all images from class 0-7 and 8/9\n",
    "x_train_0to7 = x_train[np.logical_and(y_train != 8,y_train != 9)]\n",
    "x_train_8and9 = x_train[np.logical_or(y_train == 8,y_train == 9)]\n",
    "y_train_0to7 = y_train[np.logical_and(y_train != 8,y_train != 9)]\n",
    "y_train_8and9 = y_train[np.logical_or(y_train == 8,y_train == 9)]\n",
    "\n",
    "# only use small subset of training set (#num_instances training images per class)\n",
    "num_instances = 10\n",
    "idx_8 = np.where(y_train_8and9 == 8)[0]\n",
    "idx_9 = np.where(y_train_8and9 == 9)[0]\n",
    "x_train_8and9 = x_train_8and9[np.concatenate([idx_8[0:num_instances],idx_9[0:num_instances]]),:]\n",
    "y_train_8and9 = y_train_8and9[np.concatenate([idx_8[0:num_instances],idx_9[0:num_instances]])]\n",
    "\n",
    "# get all images for the test data\n",
    "x_test_0to7 = x_test[np.logical_and(y_test != 8,y_test != 9)]\n",
    "x_test_8and9 = x_test[np.logical_or(y_test == 8,y_test == 9)]\n",
    "y_test_0to7 = y_test[np.logical_and(y_test != 8,y_test != 9)]\n",
    "y_test_8and9 = y_test[np.logical_or(y_test == 8,y_test == 9)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<b>Excercise 1</b>\n",
    "\n",
    "Write the python code to show the first 10 instances of the training data set for the problem of recognizing the digits from 0 to 7 and the first 10 instances for the second problem of recognizing the digits 8 and 9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE GOES HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model to detect handwritten digits is defined as shown in tutorial last week. Because we want to train a model to distinguish between the digits from 0 to 7 we only have 8 different classes. This model will be the basis and will be finetuned on the dataset of only two different digits 8 and 9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSimpleCNN(nb_classes=8):\n",
    "    # domain adaptation neural net\n",
    "    nb_filters_one = 32\n",
    "    nb_filters_two = 64\n",
    "    nb_conv = 3\n",
    "    nb_pool = 2\n",
    "    dense_size = 128\n",
    "    cnnModel = Sequential()\n",
    "    cnnModel.add(Conv2D(nb_filters_one, kernel_size=(nb_conv, nb_conv),\n",
    "                     activation='relu',\n",
    "                     input_shape=input_shape,name='conv'))\n",
    "    #cnnModel.add(Conv2D(nb_filters_two, (nb_conv, nb_conv), activation='relu'))\n",
    "    cnnModel.add(MaxPooling2D(pool_size=(nb_pool, nb_pool),name='max'))\n",
    "    cnnModel.add(Dropout(0.25))\n",
    "    cnnModel.add(Flatten())\n",
    "    cnnModel.add(Dense(dense_size, activation='relu',name='dense'))\n",
    "    cnnModel.add(Dropout(0.5))\n",
    "    cnnModel.add(Dense(nb_classes, activation='softmax'))\n",
    "\n",
    "    cnnModel.compile(loss='categorical_crossentropy', optimizer='SGD', metrics=['accuracy'])\n",
    "    return cnnModel\n",
    "cnnModel = getSimpleCNN()\n",
    "cnnModel.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we train our model for 10 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "encoder = LabelEncoder().fit(y_train_0to7)\n",
    "oneHotLabelTrain = np_utils.to_categorical(encoder.transform(y_train_0to7), len(np.unique(y_train_0to7)))\n",
    "oneHotLabelTest  = np_utils.to_categorical(encoder.transform(y_test_0to7), len(np.unique(y_test_0to7)))\n",
    "learnHistSimple = cnnModel.fit(x_train_0to7,oneHotLabelTrain,validation_data=(x_test_0to7,oneHotLabelTest),\n",
    "                                  batch_size=batch_size,\n",
    "                                  epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Exercise 2:</b>  \n",
    "Write code to show the learning curve and look at the learning curve. The learning curve should show the training and testing loss for the different epochs. Would you train the model for more epochs? Is the model converged? What conclusions can you draw from the learning curve below?\n",
    "\n",
    "<b>Answer:</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE GOES HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we train a simple cnn on the small data that only contains the digits 8 and 9. To do so we first of all create the cnn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnnModelSmallDataset = getSimpleCNN(2)\n",
    "cnnModelSmallDataset.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "encoder = LabelEncoder().fit(y_train_8and9)\n",
    "oneHotLabelTrain = np_utils.to_categorical(encoder.transform(y_train_8and9), len(np.unique(y_train_8and9)))\n",
    "oneHotLabelTest  = np_utils.to_categorical(encoder.transform(y_test_8and9), len(np.unique(y_test_8and9)))\n",
    "learnHistSimple = cnnModelSmallDataset.fit(x_train_8and9,oneHotLabelTrain,validation_data=(x_test_8and9,oneHotLabelTest),\n",
    "                                  batch_size=batch_size,\n",
    "                                  epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Using pretrained weights</b>\n",
    "\n",
    "If we define a neural net using keras we can initialize all the weights for the different layers with some weights of a different neural network (using the argument weights during initilization). To illustrate this look at the example above, where 3 different models are created. The first 2 models are initialized at random where the weights of the third model are the same as the weights for the first neural net."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net1 = Sequential()\n",
    "net1.add(Dense(100,input_shape=[100,]))\n",
    "net1.add(Dense(1))\n",
    "net1.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "net1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net2 = Sequential()\n",
    "net2.add(Dense(100,input_shape=[100,]))\n",
    "net2.add(Dense(1))\n",
    "net2.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "net2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net3 = Sequential()\n",
    "net3.add(Dense(100,input_shape=[100,],weights=net1.layers[0].get_weights()))\n",
    "net3.add(Dense(1,weights=net1.layers[1].get_weights()))\n",
    "net3.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "net3.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Exercise 3:</b>  \n",
    "Create 10 random test instances for the neural nets net1,net2 and net3 (a matirx with dimension 10 x 100). What is the outcome for the 3 different nets?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE GOES HERE\n",
    "testInstances = \n",
    "predict1 = \n",
    "predict2 = \n",
    "predict3 ="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Exercise 4:</b> \n",
    "\n",
    "Create a cnn like the function 'getSimpleCNN' does, but use as weights for initialization the weights of the pretrained model 'cnnModel' for the convolution, the max pooling and the first dense layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_filters_one = 32\n",
    "nb_filters_two = 64\n",
    "nb_conv = 3\n",
    "nb_pool = 2\n",
    "dense_size = 128\n",
    "nb_classes = 2\n",
    "\n",
    "cnnModelDomainAdaptation = Sequential()\n",
    "# YOUR CODE GOES HERE\n",
    "\n",
    "\n",
    "cnnModelDomainAdaptation.compile(loss='categorical_crossentropy', optimizer='SGD', metrics=['accuracy'])\n",
    "cnnModelDomainAdaptation.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we train the new model using the pretrained weights for initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "encoder = LabelEncoder().fit(y_train_8and9)\n",
    "oneHotLabelTrain = np_utils.to_categorical(encoder.transform(y_train_8and9), len(np.unique(y_train_8and9)))\n",
    "oneHotLabelTest  = np_utils.to_categorical(encoder.transform(y_test_8and9), len(np.unique(y_test_8and9)))\n",
    "learnHistSimple = cnnModelDomainAdaptation.fit(x_train_8and9,oneHotLabelTrain,validation_data=(x_test_8and9,oneHotLabelTest),\n",
    "                                  batch_size=batch_size,\n",
    "                                  epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Exercise 5:</b>\n",
    "Compare the two models trained above for recognizing the digits 8 and 9 (cnnModel vs. cnnModelDomainAdaptation) with each other. Which model would you prefer?\n",
    "\n",
    "<b>Answer:</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>2. Part: Autoencoder</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the second part of this tutorial we build an autoencoder to denoise images. To do so we are now using the functional api of keras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "First of all we create a noisy data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<b>Exercise 6:</b>\n",
    "Create a noisy dataset by adding noise to all the images from the training and testing data 'x_train' and 'x_test', respectively. Your dataset should then look like shown in the figure below.\n",
    "\n",
    "<img src=\"files/noisy.png\",width=600,height=600>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# YOUR CODE GOES HERE\n",
    "#x_train_noisy = \n",
    "#x_test_noisy = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, test your noising code by looking at the first 10 images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show images\n",
    "n = 10\n",
    "plt.figure(figsize=(20, 2))\n",
    "for i in range(n):\n",
    "    ax = plt.subplot(1, n, i+1)\n",
    "    plt.imshow(x_test_noisy[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now, lets define the the simplest autoencoder we can imagine. Here we try to encode the images with 32 floats. The input are the raw pixel values for the images as a flattened vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this is the size of our encoded representations\n",
    "encoding_dim = 32  # 32 floats -> compression of factor 24.5, assuming the input is 784 floats\n",
    "\n",
    "# this is our input placeholder\n",
    "input_img = Input(shape=(784,))\n",
    "# \"encoded\" is the encoded representation of the input\n",
    "encoded = Dense(encoding_dim, activation='relu')(input_img)\n",
    "# \"decoded\" is the lossy reconstruction of the input\n",
    "decoded = Dense(784, activation='sigmoid')(encoded)\n",
    "\n",
    "# this model maps an input to its reconstruction\n",
    "autoencoder = Model(input_img, decoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the encoder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this model maps an input to its encoded representation\n",
    "encoder = Model(input_img, encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the decoder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a placeholder for an encoded (32-dimensional) input\n",
    "encoded_input = Input(shape=(encoding_dim,))\n",
    "# retrieve the last layer of the autoencoder model\n",
    "decoder_layer = autoencoder.layers[-1]\n",
    "# create the decoder model\n",
    "decoder = Model(encoded_input, decoder_layer(encoded_input))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we compile and train the autoencoder on the flattened images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "autoEncoderHist = autoencoder.fit(x_train_noisy.reshape(x_train_noisy.shape[0],28*28),\n",
    "                x_train_noisy.reshape(x_train_noisy.shape[0],28*28),\n",
    "                epochs=10,\n",
    "                batch_size=128,\n",
    "                shuffle=True,\n",
    "                validation_data=(x_test_noisy.reshape(x_test_noisy.shape[0],28*28),\n",
    "                                 x_test_noisy.reshape(x_test_noisy.shape[0],28*28)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# encode and decode some digits\n",
    "encoded_imgs = encoder.predict(x_test_noisy.reshape(x_test_noisy.shape[0],28*28))\n",
    "decoded_imgs = decoder.predict(encoded_imgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Exercise 7:</b>\n",
    "Plot the first 10 images from the noisy test set and their denoised prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE GOES HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can we do better when giving the exact objective of trying to build a model the creates for a noisy input its denoised version?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the size of our encoded representations\n",
    "encoding_dim = 32  # 32 floats -> compression of factor 24.5, assuming the input is 784 floats\n",
    "\n",
    "# this is our input placeholder\n",
    "input_img = Input(shape=(784,))\n",
    "# \"encoded\" is the encoded representation of the input\n",
    "encoded = Dense(encoding_dim, activation='relu')(input_img)\n",
    "# \"decoded\" is the lossy reconstruction of the input\n",
    "decoded = Dense(784, activation='sigmoid')(encoded)\n",
    "\n",
    "# this model maps an input to its encoded representation\n",
    "encoder = Model(input_img, encoded)\n",
    "\n",
    "# create a placeholder for an encoded (32-dimensional) input\n",
    "encoded_input = Input(shape=(encoding_dim,))\n",
    "# retrieve the last layer of the autoencoder model\n",
    "decoder_layer = autoencoder.layers[-1]\n",
    "# create the decoder model\n",
    "decoder = Model(encoded_input, decoder_layer(encoded_input))\n",
    "\n",
    "# this model maps an input to its reconstruction\n",
    "autoencoder = Model(input_img, decoded)\n",
    "autoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "autoencoder.summary()\n",
    "\n",
    "autoEncoderHist = autoencoder.fit(x_train_noisy.reshape(x_train_noisy.shape[0],28*28),\n",
    "                x_train.reshape(x_train_noisy.shape[0],28*28),\n",
    "                epochs=10,\n",
    "                batch_size=128,\n",
    "                shuffle=True,\n",
    "                validation_data=(x_test_noisy.reshape(x_test_noisy.shape[0],28*28),\n",
    "                                 x_test.reshape(x_test.shape[0],28*28)))\n",
    "\n",
    "\n",
    "\n",
    "# encode and decode some digits\n",
    "encoded_imgs = encoder.predict(x_test_noisy.reshape(x_test_noisy.shape[0],28*28))\n",
    "decoded_imgs = decoder.predict(encoded_imgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Excercise 8:</b>\n",
    "\n",
    "Add your code from Excersice 7 and look at the new results. Are they better now? And if yes, why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE GOES HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<b>Additional Exercise:</b>\n",
    "\n",
    "Try to build a autoencoder using CNN's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
